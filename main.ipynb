{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5dab53",
   "metadata": {},
   "source": [
    "# MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks\n",
    "https://arxiv.org/pdf/1903.06048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da749da0",
   "metadata": {},
   "source": [
    "Implementation by Deniz A. ACAR, Yavuz DURMAZKESER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d319e3",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Here we present the implementation of the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN) a simple but effective technique for addressing GAN training problems. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique (ProGAN [2] and StyleGAN [3]).</p>\n",
    "\n",
    "The authors claim that `\"MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters.\"`\n",
    "\n",
    "***\n",
    "\n",
    "### Motivation\n",
    "\n",
    "<p style=\"text-align: justify\">Arjovsky and Bottou [4] pointed out that one of the reasons for the training instability of GANs is due to the\n",
    "passage of random (uninformative) gradients from the discriminator to the generator when there is insubstantial overlap between the supports of the real and fake distributions.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Peng et al. [5] proposed a mutual information bottleneck between input images and the discriminator’s deepest representation of those input images called the variational discriminator bottleneck (VDB) [6], and Karras et al.[7] proposed a progressive growing technique to add continually increasing resolution layers. The VDB solution forces the discriminator to focus only on the most discerning features of the images for classification, which can be viewed as an adaptive variant of instance noise. The progressive growing technique tackles the instability problem by training the GAN layer-by-layer by gradually doubling the operating resolution of the generated images. Whenever a new layer is added to the training it is slowly faded in such that the learning of the previous layers are retained. Intuitively, this technique helps with the support overlap problem because it first achieves a good distribution match on lower resolutions, where the data dimensionality is lower, and then partially-initializes (with substantial support overlap between real and fake distributions) higher resolution training with these previously trained weights, focusing on learning finer details.\n",
    "</p>\n",
    "<p style=\"text-align: justify\">\n",
    "While this approach is able to generate state-of-the-art results, it can be hard to train, due to the addition of hyperparameters to be tuned per resolution, including different iteration counts, learning rates (which can be different for the Generator and Discriminator [8]) and the fade-in iterations. In addition, a concurrent submission [9] discovered that it leads to phase artifacts where certain generated features are attached to specific spatial locations. Hence the main motivation in this paper lies in addressing these problems by providing a simpler alternative that leads to high quality results and stable training.\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "The idea behind MSG-GAN is presented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b61b7",
   "metadata": {},
   "source": [
    "<img src=\"doc/schematic_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea495c",
   "metadata": {},
   "source": [
    "Architecture of MSG-GAN, shown here on the base model proposed in ProGANs which is implemented in this project.\n",
    "\n",
    "***\n",
    "\n",
    "### Multi-Scale Gradient GAN\n",
    "\n",
    "#### Generator Architecture\n",
    "***\n",
    "Let the initial block of the generator function $g_{gen} = A_{begin}$ where $A_{begin} \\in \\mathbb{R}^{4 \\times4\\times512}$ contains [4x4x512] dimensional activations. Let $g^i$ be a generic function which acts as the basic generator block where in this implementation consists of an upsampling operation followed by two conv layers.\n",
    "\n",
    "<img src=\"doc/generator_eqn.png\" style=\"width: 450px;\">\n",
    "\n",
    "where $c_i$ is the number of channels in the $i^{th}$ intermediate activations of the generator. The full generator is as follows:\n",
    "\n",
    "<img src=\"doc/full_generator.png\" style=\"width: 700px;\">\n",
    "\n",
    "Where $z~\\mathcal{N}(0, \\mathbb{I})$.\n",
    "\n",
    "The generator architecture is presented below:\n",
    "\n",
    "<img src=\"doc/generator_schema.png\" style=\"width: 300px;\">\n",
    "\n",
    "***\n",
    "\n",
    "##### Generator output for different stages\n",
    "\n",
    "The r is defined such that it generates the output RGB at different stages of the generator where its output corresponds to different downsampled versions of the final output image. It is simply a $(1\\times1)$ convolution which converts the intermediate convolutional activation volume into images.\n",
    "\n",
    "<img src=\"doc/toRGB.png\" style=\"width: 500px;\">\n",
    "\n",
    "In other words, $o_i$ is an image synthesized from the output of the $i^{th}$ intermediate layer of the generator $a_i$ similar to the idea behind progressive growing.\n",
    "\n",
    "***\n",
    "\n",
    "#### Discriminator Architecture\n",
    "\n",
    "Because the discriminator’s final critic loss is a function of not only the final output of the generator y', but also the intermediate outputs $o_i$, gradients can flow from the intermediate layersof the discriminator to the intermediate layers of the generator.\n",
    "\n",
    "Here all components of the the discriminator are denoted by `\"d\"`. \n",
    "The final layer of the discriminator is called $d_{critic}(z')$ and the function which defines the first layer of the discriminator is denoted by $d^0(y)$ or $d^0(y)$ taking the real image y (true sample) or the highest resolution synthesized image y' (fake sample) as the input. \n",
    "\n",
    "Similarly, let $d^j$ represent the intermediate layer function of the discriminator. Note that i and j are always related to each other as j = k−i. Thus, the output activation volume $a'_j$ of any $j^{th}$ intermediate layer of the discriminator is defined as:\n",
    "\n",
    "<img src=\"doc/disc_eqn.png\" style=\"width: 450px;\">\n",
    "\n",
    "where $\\phi$ is a function used to combine the output oi of the $i^{th}$ intermediate layer of the generator (or correspondingly downsampled version of the highest resolution real image y) with the corresponding output of the (j − 1)th intermediate layer in the discriminator.\n",
    "\n",
    "There are three different varients of the $\\phi$ which are as follows:\n",
    "\n",
    "<img src=\"doc/phi.png\" style=\"width: 400px;\">\n",
    "\n",
    "where, r' is yet another (1x1) convolution operation similar to r and [;] is a simple channelwise concatenation operation.\n",
    "\n",
    "The full discriminator is as follows:\n",
    "\n",
    "<img src=\"doc/discriminator_full.png\" style=\"width: 700px;\">\n",
    "\n",
    "The discriminator architecture is represented below:\n",
    "\n",
    "<img src=\"doc/disc_architecture.png\">\n",
    "\n",
    "***\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "<p style=\"text-align: justify\">They used two different loss functions for the $d_{critic}$ function namely, WGAN-GP [10] which wasused by ProGAN [2] and Non-saturating GAN loss with 1-sided GP [11] which was used by StyleGAN [3]. One thing to note that is the fact that the discriminator is now a function of multiple input images generated by the generator, thus gradient penalty is modified to be the average of the penalties over each input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7b177",
   "metadata": {},
   "source": [
    "***\n",
    "### Implementation Notes\n",
    "\n",
    "There two different architectures discussed in the paper in accordance with the technique presented here. We have selected ProGANs based architecture to implement for the course project which was also in accordance with our selected qualitative and quantitative results. We have consulted with the instructor of the course and decided to focus on the aforementioned architecture. As the loss function of ProGANs is WGAN-GP is implemented here.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7d2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d396ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
